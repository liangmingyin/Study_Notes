# spark概述

MapReduce、Spark、Flink（实时） => 3代计算引擎；**昨天、今天、未来**

MapReduce、Spark：类MR的处理引擎；底层原理非常相似；数据分区、map、task、reduce task、shuffle

## 1、什么是Spark

Spark是当今大数据领域最活跃、最热门、最高效的大数据通用计算引擎

2009年诞生于美国加州大学伯克利分校AMP 实验室

2010年通过BSD许可协议开源发布

2013年捐赠给Apache软件基金会并切换开源协议到切换许可协议至 Apache2.0

2014年2月，Spark 成为 Apache 的顶级项目

2014年11月, Spark的母公司Databricks团队使用Spark刷新数据排序世界记录

Spark 成功构建起了一体化、多元化的大数据处理体系。在任何规模的数据计算中，Spark 在性能和扩展性上都更具优势



**Spark 是一个快速、通用的计算引擎。Spark的特点：**

**速度快:**  与 MapReduce 相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流；

**使用简单:**Spark支持 Scala、Java、Python、R的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法；

**通用:** Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询(Spark SQL)、实时流处理(Spark Streaming)、机器学习(Spark MLlib)和图计算(GraphX)。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，企业想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本；

**兼容好:**  Spark可以非常方便地与其他的开源产品进行融合。Spark可以使用YARN、Mesos作为它的资源管理和调度器；可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。



## 1.2、**Spark** **与** **Hadoop**



从狭义的角度上看：Hadoop是一个分布式框架，由**存储、资源调度、计算**三部分组成；

Spark是一个分布式计算引擎，由 Scala 语言编写的计算框架，基于内存的快速、通用、可扩展的大数据分析引擎；

从广义的角度上看，Spark是Hadoop生态中不可或缺的一部分；

**MapReduce的不足：**

-  表达能力有限

-  磁盘IO开销大

-  延迟高

   1、 任务之间的衔接有IO开销

​                 2、在前一个任务执行完成之前，后一个任务无法开始。难以胜任复杂的、多阶段计算任务


![图片](https://user-images.githubusercontent.com/53028208/133748800-adf3cf61-8993-4ae6-ba0a-131520f89918.png)
